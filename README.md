# 職務経歴書
## プロフィール
- 名前：熊切俊夫
- ID：tkumakiri

## 職務経歴
### パーソルキャリア株式会社（現在）
新規サービス領域（1 → 10）の開発に従事。主に2つのサービス開発を担当

◾️ BtoBサービス1のフロントエンド開発
- Nuxt2からNuxt3のバージョンアップ対応
  - 背景：管理画面でNuxt2を使用していたが、サポート期限が迫っていた。サポート切れによりセキュリティリスクが増大するため速やかなバージョンアップが必要であった。
  - 取り組み
    - 新卒配属直後にバージョンアップ対応の計画から詳細タスク化・実際の移行まで一気通貫で完遂
    - チームメンバーは既存機能の開発で手一杯で主体的かつ自律的に実行
  - 成果
    - 2ヶ月でバージョンアップからの安定稼働まで完遂
    - 管理が行き届いていなかった管理構成・詳細ドキュメント化によるナレッジ展開
    - ユーザー向けサービスの継続開発を緩めずリリース
- Next.jsでの機能開発・UI改善
- サービスクローズ作業の経験
  - 背景：運営方針に基づきサービスクローズの運びとなった。これまでの運用では発生しないオペレーションに対して調査実装まで行った
  - 取り組み：全ユーザーの解約処理実行に伴いメール配信処理の負荷テストの実施。運用方針の策定。
  - 成果：大量解約処理のボトルネックがSendGridと解約処理を同一トランザクションで実施したことが原因を突き止める。本番環境でのエラーなく完遂。
使用技術：Next.js, React, TypeScript, Nuxt, Vue, MUI, SendGrid

◾️ BtoBサービス2の開発
#### 概要
- フロントエンドからバックエンド・インフラ領域まで従事
- メインはGoogle Cloudを用いたインフラ管理・構築をサービス全体でリード

#### 詳細 
- Next.js App RouterでSerever Component・Server Actionをベースとした開発
  - 背景
    - チーム体制が全員がフルスタックエンジニアとして動いていた -> メンバー依存の排除
    - 新規にチャット機能を開発しており、そこに導入した
    - チャット機能はモノレポで構成されていた
  - 成果
    - モノレポ x フルスタックチームとの相性でコード管理の効率化
    - チャットというUXが重要なサービスにおける初期レンダリングの高速化
  - 振り返り
    - 社内で技術的挑戦ができる場所が少なく組織全体の技術力向上も踏まえて選定 ->  結果的には良かったがもう少し踏み込んだ技術選定があっても良かった
    - チーム体制の相性が良かったが大規模構築になると要検討
- NestJS → Honoのサーバー移管
  - 背景：チャット機能におけるメッセージ送信機能は別のAPIサーバーを構築しておりそこにNestJSを採用していたが、規模感が噛み合わずリッチな構成が逆に開発効率を下げていた
  - 成果
    - シンプルな構成による開発効率の向上
    - TypeScriptとの親和性による型安全性の確保
- 認証サーバーの0から構築
  - 背景
    - 社内サービス間統合の動きがある中、各サービスではそれぞれの認証方法を使用していた
    - 従事していたPJでは統一されたfirebase authenticationを使用していたため、依存が発生していた。
    - 将来的な認証統一に向け予めPJ内で統一した認証サーバを持つことでスムーズなサービス間統合に備えた
  - 取り組み：cookie認証をHonoで実装
  - 成果：第三者機関の脆弱性診断での検出0で完遂
- Google Cloudのフロントエンドリソースを別プロジェクトにダウンタイムなしで移行
  - 背景
    - 当時バックエンドチームとフロントエンドチームが部署単位で分離されていた
    - バックエンドチームがフロントエンドリソースを含むインフラ管理を実施していた
    - バックエンドチームが3人に対しフロントエンドチームは10人以上おりチームバランスに偏りがあった
    - 部署間の管理方法が開発環境に依存しており、その影響でフロントエンドチームがインフラを触るには5分程度接続準備が必要であった
    - フロントエンドチームは自分たちの使用するリソースを自分たちで管理することで責任境界を明確化し、開発効率を上げることが目的
  - 取り組み
    - 当時フロントエンドチームにはインフラに明るい人材が一人もいない中で主体性を持って計画から運用まで全て担当
    - これまでPJで導入されていなかったterraformを用いて新規環境を構築
    - Load Balancingのterraformモジュールを独自で構築しPJ特有のマイクロサービス間ルーティングを変数のみで管理可能なように構築
    - Google Cloudのマネージド証明書をDNS認証を採用することでダウンタイムなしで移行
  - 成果
    - エラーなく完遂
    - インフラの責任分散を達成し管理可能な人数を3倍以上に増加
    - アクセスまで5分要していたものを数秒で閲覧可能
    - 環境閲覧が容易になり、Cloud Armorの余分コストを発見。年間で\$3600のコストカットに貢献。
- Google CloudのOrgを跨いだプロジェクト移行
  - 背景
    - 年度を挟み部署再編によりバックエンドメンバーがより縮小 3人->1人(この1人も元々フロントエンドメンバー)
    - フロントエンドメンバーの偏りを環境だけでなくバックエンドメンバーに割り振る必要が出てきた
    - バックエンドのコードも去ることながら依然としてインフラ管理は管理組織間の課題となっており参入障壁となっていた
    - 長年従事していたメンバーの離脱により、ナレッジが消失しインフラ全体を理解できている人がPJ内で不在となった
  - 取り組み
    - Google CloudのOrgをフロントエンドメンバーが所属する部署に移管した
    - Org管理者でない私が手順書を作成し管理者に依頼
    - インフラ構成の詳細をドキュメント化
    - IAMの基準を制定し誤操作防止かつリソース閲覧の即時性を追求
  - 成果
    - 全環境において即時環境アクセスを可能
    - システム全体の管理可能な人数を3倍以上に増加->監視体制の強化
    - バックエンドのアプリケーションレイヤの染み出ししやすい環境構築
    - チーム全体にインフラ知見の共有のきっかけ
- Pythonを用いた0からAPIサーバーの構築
- Dataflowを用いた大規模データ処理の構築
  - 背景
    - 社内コンプラ要件でデータのマスキングをする必要が出てきた
    - サービスでは億単位のレコードを扱っていた
    - データ管理にはCloud SQLを使用していたが、すでにバッチ処理時間だけで3時間要していたためCloud SQLでの操作は困難であった。
    - 別部署で管理しているCSVのgz圧縮をしているデータを取り扱っていた
  - 取り組み
    - 大規模処理に向いているDataflowを使用したマスキングを実施
    - Cloud SQLに取り込む前にCSVデータに対してマスキングを検証環境のみ実施
    - カスタムコンテナとFlexテンプレートで実行環境の明示化
  - 成果
    - Dataflowのパイプライン構成の工夫により1時間24分から56分に短縮
    - Cloud SQLとの分離より可用性の担保をする構成を構築
  - 振り返り
    - データ形式がgzだとそもそも遅いのでこの規模になるとParquetのようなデータ構成の検討が必要
    - 部署意向でマスキングより他機能を優先したため未実装
- PAMを用いたGoogle Cloudプロジェクトのセキュアな環境構築
  - 背景
    - 通常時は誤操作防止のため閲覧権限に留めているが、障害時等突発的な対応には一時的に権限を昇格する必要がある
    - この権限昇格では承認履歴を残すことで監査ログとして追いやすい
  - 取り組み
    - 起案・計画から運用まで全て担当
    - Slackと連携し申請時・承認時・有効期限切れ時には通知
  - 成果
    - PAMの導入によりIAM管理を厳密化し誤操作による障害を防止する環境を構築
    - 承認者をMGRレイヤにすることで組織的な責任体型をシステムで構築
- SendGridを用いたメール送信機構の構築
- 社内コンプライアンス事情を網羅的に把握しセキュリティアセスメントへの回答及び環境のブラッシュアップ（データマスキング処理の実装等）
- Cloud Functions V1からCloud Runへのバックエンドサーバー移管
  - 背景
    - バックエンドサーバーをCloud Functions V1 x Goで構築していた
    - Goのバージョンを上げるごとにサーバー側のメンテナンスも必要
    - サポートバージョンがGoogle Cloudに握られているため都度対応が必要。対応時も非推奨が直近に迫っていた
    - Cloud Runのサポートが厚く今後のサービスグロースを考えると移行が適していた
  - 取り組み
    - 起案・計画から運用まで全て担当
    - Goのビルドコードを解読しアプリケーション用のDockerfileの構築
  - 成果
    - Dockerによる環境差異の低減
    - リリース期日を1ヶ月前に通達されるも遅延・エラーなく完遂
    - インフラレイヤから必要なパッケージ（エクセル構築等）をDocker単位でカスタマイズを可能にした
    - Serverless VPC Access ConnectorからDirect VPC Egressを活用するようになったためコストカットに貢献した
- 大規模データアーキテクチャを念頭に入れたBigQuery活用の調査・検討
  - 背景
    - 社内向けの新規機能開発でこれまで以上に大規模なデータを取り扱うことになった
    - Cloud SQLで検討したところパフォーマンスが出ず夜間バッチで4時間かかっており、さらに更新処理で追加時間がかかった
    - BigQuery導入によりパフォーマンス改善を主の目的としたリアーキテクチャを実施した
  - 取り組み
    - 起案・計画から運用まで全て担当
    - Parquetデータへの変更による処理の更なる高速化
    - Dataformを使用したコードレビュー体制を整えつつ自動更新処理の実装・メンバーのレビュー
    - 日時更新バッチの実装
  - 成果
    - 4時間かかっていたバッチ処理を12分に短縮 -> 障害時の切り戻し・復旧が迅速化
    - API実行時間を6分かかっていたものを6秒に短縮 -> 社内2000人弱が業務で使用するツールのパフォーマンスを改善
    - 当初作成していたクエリを改善することでコストを1/40に削減

◾️ Snowflakeを活用したデータ活用促進化
- Snowflake intelligence活用したデータ接点の増加
  - 背景：社内ではデータ活用についてまだ改善の余地がある。その接点としてAIを活用したSQL知識がなくてもレポート化するSnowflake Intelligenceが存在する
  - 取り組み
    - Cortex Search, Cortex Analystを活用したAIエージェントの構築
    - メタデータ調整による精度検証 


◾️ 組織開発
- 勉強会の運営
- Github Copilotのナレッジ集約・tips共有
- ADKを用いた社内ドキュメント検索AIエージェントの構築を起案し実装
- n8nを用いた組織関連のニュース要約アプリ（Gemini）を起案し実装
- クローズ判断されたサービスの計画・コミュニケーション・対応を全て担当

## スキル
### 技術スタック
- Typescript
- Next.js, React
- Vue, Nuxt
- NestJS, Hono
- Google Cloud
- Terraform
- Python
- Go

### 経験・知識
- フロントエンドからインフラまでの経験
- 設計から実装までやり遂げる能力
- 学生時代のAI研究で培ったAIへの理解・基礎的な統計
- 効率的なコミュニケーション・リーダーシップ
- ADKやRAG構築など内発的動機から学ぶ最新技術

## 大事にしていること
**ユーザーへ価値をなるべく早く最適な形で提供する**
- アウトプットではなくアウトカムを重視
- 最新技術ではなく適切な技術選定が重要

私が今目指しているエンジニア像は以下と定義しています。
- ユーザーを理解する：ユーザー行動からなるべく多くの情報を取得できるような設計・提案
- 早く価値を届ける：企画段階における実現可能性検証から染み出してデリバリー効率化を目指す
- フィードバックを反映する：ログ・VoCから得た情報をもとに素早く実装する
- チームでプロダクトを作る：コミュニケーションを欠かさない。チームで成果を出す

### 行動
- Backlog・Github Issueの綿密な更新
- 企画・マーケ・CSチーム/社外の担当者との積極的なコミュニケーション
- 必要に感じた内容をボトムアップ的に起案し実装までやり切る


## 記事
- https://techtekt.persol-career.co.jp/entry/culture/20231219_01
- https://techtekt.persol-career.co.jp/entry/tech/241220_01
- https://techtekt.persol-career.co.jp/entry/tech/251215_02
- https://techtekt.persol-career.co.jp/entry/culture/251211_02

## やりたいこと
```
データを起点としたKPIに貢献できるプロダクト開発
```

学生時代にAI開発に取り組んでいた背景もあり、データ領域は興味があります。
新卒から一貫してフルスタックにサービスをデリバリー・運用する経験を積んだので、今後はそれをデータ領域と接続してより強固な価値提供者になりたいと考えています。
特に仮説検証からチーム全体で取り組みエンジニアは作るだけに留まらず、データの解読・整備を実施しエンジニアからも提案できることが理想です。
MLOps等も興味領域です。

### 今後チャレンジしていきたいこと
- KPIをより意識したものづくりの視点
  - これまでモノづくりのhowについては学んできたので今後はそれをよりビジネス価値として貢献するフェーズとして捉えています。
  - そのためにはその施策に対する費用対効果を自ら率先していく必要があると思います。
  - 最近はコストやパフォーマンスを定量的にはかるように動いてきましたがそこに更なるブラッシュアップが必要だと感じています。

## （追記）学生時代の取り組み
医療画像を対象としたDeep Learningモデルを構築する研究に従事。

### 成果
- https://pubmed.ncbi.nlm.nih.gov/36944832/
- https://pubmed.ncbi.nlm.nih.gov/35622229/

